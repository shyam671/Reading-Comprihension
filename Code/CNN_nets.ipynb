{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten,Merge\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated2/train_label_.txt\", \"r\") as ins:\n",
    "    label = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        label.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 98\n",
    "GLOVE_DIR = \"\"\n",
    "EMBEDDING_DIM = 200\n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated2/train_s_.txt\", \"r\") as ins:\n",
    "    texts = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        texts.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PN_label = []\n",
    "PN_text = []\n",
    "Count = 0 \n",
    "\n",
    "for i in range(0,len(label)/4):\n",
    "    if (label[i])==2 or (label[i])==3:\n",
    "        Count = Count + 1\n",
    "        PN_label.append(label[i])\n",
    "        PN_text.append(texts[i])\n",
    "    if Count == 100000:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#maxi = 0\n",
    "#for i in range(0,len(PN_label)):\n",
    "#      if(maxi<len(PN_text[i])): \n",
    "#         maxi =  len(PN_text[i])\n",
    "#print maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(PN_text)\n",
    "sequences = tokenizer.texts_to_sequences(PN_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "maxi = 0\n",
    "for i in range(0,len(PN_label)):\n",
    "      if(maxi<len(sequences[i])): \n",
    "         maxi =  len(sequences[i])\n",
    "print maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = np.asarray(PN_label)  \n",
    "label[:] = [x - 2 for x in label] \n",
    "label = to_categorical(label)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"/home/shyam/Downloads/MS/Sem1/NLP/Project/glove/glove.6B/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count1 = count1 +1\n",
    "    else:\n",
    "        count2 = count2 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14622 1267\n"
     ]
    }
   ],
   "source": [
    "print count1,count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=4, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=5, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - simplified convolutional neural network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 98)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 98, 200)       3178000     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 96, 128)       76928       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 95, 128)       102528      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 94, 128)       128128      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 19, 128)       0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 19, 128)       0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 18, 128)       0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 56, 128)       0           max_pooling1d_1[0][0]            \n",
      "                                                                   max_pooling1d_2[0][0]            \n",
      "                                                                   max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 52, 128)       82048       merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 26, 128)       0           conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 22, 128)       82048       max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 4, 128)        0           conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           65664       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             258         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,715,602\n",
      "Trainable params: 537,602\n",
      "Non-trainable params: 3,178,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "lr = 0.0001\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr, decay = lr*0.9),\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()\n",
    "#model.fit(data,label,validation_split=0.01,nb_epoch=7, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated2/test_label_.txt\", \"r\") as ins:\n",
    "    label = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        label.append(int(line))\n",
    "        \n",
    "MAX_SEQUENCE_LENGTH = 98\n",
    "GLOVE_DIR = \"\"\n",
    "EMBEDDING_DIM = 200\n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated2/test_s_.txt\", \"r\") as ins:\n",
    "    texts = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        texts.append(line)\n",
    "PN_label = []\n",
    "PN_text = []\n",
    "Count = 0 \n",
    "\n",
    "for i in range(0,len(label)):\n",
    "    if (label[i])==2 or (label[i])==3:\n",
    "        Count = Count + 1\n",
    "        PN_label.append(label[i])\n",
    "        PN_text.append(texts[i])\n",
    "sequences = tokenizer.texts_to_sequences(PN_text)\n",
    "label = np.asarray(PN_label)  \n",
    "label[:] = [x - 2 for x in label] \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Prediction =  np.argmax(model.predict([data]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5069\n"
     ]
    }
   ],
   "source": [
    "print sum(label==Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725965177895534"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5103.0/6605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
