{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten,Merge,dot,multiply,concatenate,Lambda,Dropout\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import model_from_json\n",
    "MAX_SEQUENCE_LENGTH = 78\n",
    "GLOVE_DIR = \"\"\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/train_s1_.txt\", \"r\") as ins:\n",
    "    texts_s1 = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        texts_s1.append(line)\n",
    "        \n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/train_s2_.txt\", \"r\") as ins:\n",
    "    texts_s2 = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        texts_s2.append(line)\n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/train_label_.txt\", \"r\") as ins:\n",
    "    label = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        label.append(int(line))       \n",
    "        \n",
    "PN_label = []\n",
    "PN_text_s1 = []\n",
    "PN_text_s2 = []\n",
    "Count = 0;\n",
    "for i in range(0,len(label)):\n",
    "    if (label[i])==2 or (label[i])==3:\n",
    "        Count = Count + 1\n",
    "        PN_label.append(label[i])\n",
    "        PN_text_s1.append(texts_s1[i])\n",
    "        PN_text_s2.append(texts_s2[i])\n",
    "    if Count == 100000:\n",
    "        break;\n",
    "print len(PN_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated2/train_s_.txt\", \"r\") as ins:\n",
    "    texts = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        texts.append(line)\n",
    "        \n",
    "PN_text = []\n",
    "for i in range(0,len(label)/2):\n",
    "    if (label[i])==2 or (label[i])==3:\n",
    "        PN_text.append(texts[i])\n",
    "        \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(PN_text)\n",
    "\n",
    "GLOVE_DIR = \"/home/shyam/Downloads/MS/Sem1/NLP/Project/glove/glove.6B/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        count1 = count1 +1\n",
    "    else:\n",
    "        count2 = count2 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences1 = tokenizer.texts_to_sequences(PN_text_s1)\n",
    "sequences2 = tokenizer.texts_to_sequences(PN_text_s2)\n",
    "\n",
    "data1 = pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data2 = pad_sequences(sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "label = np.asarray(PN_label)  \n",
    "label[:] = [x - 2 for x in label] \n",
    "label = to_categorical(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/test_s1_.txt\", \"r\") as ins:\n",
    "    dev_texts_s1 = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        dev_texts_s1.append(line)\n",
    "        \n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/test_s2_.txt\", \"r\") as ins:\n",
    "    dev_texts_s2 = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        dev_texts_s2.append(line)\n",
    "with open(\"/home/shyam/Downloads/MS/Sem1/NLP/Project/Data/Seprated/test_label_.txt\", \"r\") as ins:\n",
    "    dev_label = []\n",
    "    for line in ins:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('\\t')\n",
    "        dev_label.append(int(line))\n",
    "\n",
    "dev_PN_label = []\n",
    "dev_PN_text_s1 = []\n",
    "dev_PN_text_s2 = []\n",
    "\n",
    "for i in range(0,len(dev_label)):\n",
    "    if (dev_label[i])==2 or (dev_label[i])==3:\n",
    "        dev_PN_label.append(dev_label[i])\n",
    "        dev_PN_text_s1.append(dev_texts_s1[i])\n",
    "        dev_PN_text_s2.append(dev_texts_s2[i])\n",
    "dev_sequences1 = tokenizer.texts_to_sequences(dev_PN_text_s1)\n",
    "dev_sequences2 = tokenizer.texts_to_sequences(dev_PN_text_s2)\n",
    "\n",
    "dev_data1 = pad_sequences(dev_sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "dev_data2 = pad_sequences(dev_sequences2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "dev_label = np.asarray(dev_PN_label)  \n",
    "dev_label[:] = [x - 2 for x in dev_label] \n",
    "#dev_label = to_categorical(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _differences(pair_of_tensors):\n",
    "    x, y = pair_of_tensors\n",
    "    return (x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=2, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=4, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=5, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=2, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=4, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=5, filters=128)`\n",
      "/home/shyam/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:35: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "Input1 = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "Input2 = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "\n",
    "sentence1 = Embedding(len(word_index) + 1,EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(Input1)\n",
    "\n",
    "sentence2 = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(Input2)\n",
    "\n",
    "convs_s1 = []\n",
    "filter_sizes = [2,3,4,5]\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv_s1 = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(sentence1)\n",
    "    #l_conv_s1 = Dropout(0.2)(l_conv_s1)\n",
    "    l_pool_s1 = MaxPooling1D(5)(l_conv_s1)\n",
    "    convs_s1.append(l_pool_s1)\n",
    "\n",
    "l_merge_s1 = Merge(mode='concat', concat_axis=1)(convs_s1)\n",
    "#l_merge_s1 = BatchNormalization()(l_merge_s1)\n",
    "l_cov1_s1= Conv1D(128, 5, activation='relu')(l_merge_s1)\n",
    "l_pool1_s1 = MaxPooling1D(5)(l_cov1_s1)\n",
    "#l_pool1_s1 = BatchNormalization()(l_pool1_s1)\n",
    "l_cov2_s1 = Conv1D(192, 5, activation='relu')(l_pool1_s1)\n",
    "l_pool2_s1 = MaxPooling1D(5)(l_cov2_s1)\n",
    "#l_pool2_s1 = BatchNormalization()(l_pool2_s1)\n",
    "convs_s2 = []\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv_s2 = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(sentence2)\n",
    " #   l_conv_s2 = Dropout(0.2)(l_conv_s2)\n",
    "    l_pool_s2 = MaxPooling1D(5)(l_conv_s2)\n",
    "    convs_s2.append(l_pool_s2)\n",
    "\n",
    "l_merge_s2 = Merge(mode='concat', concat_axis=1)(convs_s2)\n",
    "#l_merge_s2 = BatchNormalization()(l_merge_s2)\n",
    "l_cov1_s2= Conv1D(128, 5, activation='relu')(l_merge_s2)\n",
    "l_pool1_s2 = MaxPooling1D(5)(l_cov1_s2)\n",
    "#l_pool1_s2 = BatchNormalization()(l_pool1_s2)\n",
    "l_cov2_s2 = Conv1D(192, 5, activation='relu')(l_pool1_s2)\n",
    "l_pool2_s2 = MaxPooling1D(5)(l_cov2_s2)\n",
    "#l_pool2_s2 = BatchNormalization()(l_pool2_s2)\n",
    "\n",
    "merge_m = multiply([l_cov2_s2,l_cov2_s1])\n",
    "merge_d = Lambda(_differences)([l_cov2_s2, l_cov2_s1])\n",
    "merge_s = concatenate([merge_d,merge_m],axis = -2)\n",
    "#merge_s = BatchNormalization()(merge_s)\n",
    "merge_s = Flatten()(merge_s)\n",
    "merge_s = Dense(2048, activation='relu')(merge_s)\n",
    "merge_s = Dropout(0.1)(merge_s)\n",
    "merge_s = Dense(512, activation='relu')(merge_s)\n",
    "merge_s = Dropout(0.05)(merge_s)\n",
    "merge_s = Dense(84, activation='relu')(merge_s)\n",
    "preds = Dense(2, activation='softmax')(merge_s)\n",
    "\n",
    "\n",
    "model = Model(inputs=[Input1,Input2], outputs=preds)\n",
    "lr = 0.0003\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr, decay = lr*0.95),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/7\n",
      "99000/99000 [==============================] - 1960s - loss: 0.5485 - acc: 0.7081 - val_loss: 0.4863 - val_acc: 0.7480\n",
      "Epoch 2/7\n",
      "99000/99000 [==============================] - 2102s - loss: 0.4684 - acc: 0.7671 - val_loss: 0.4585 - val_acc: 0.7720\n",
      "Epoch 3/7\n",
      "99000/99000 [==============================] - 1979s - loss: 0.4251 - acc: 0.7962 - val_loss: 0.4522 - val_acc: 0.7710\n",
      "Epoch 4/7\n",
      "99000/99000 [==============================] - 1955s - loss: 0.3773 - acc: 0.8248 - val_loss: 0.4316 - val_acc: 0.8000\n",
      "Epoch 5/7\n",
      "99000/99000 [==============================] - 1957s - loss: 0.3322 - acc: 0.8508 - val_loss: 0.4273 - val_acc: 0.8030\n",
      "Epoch 6/7\n",
      "99000/99000 [==============================] - 1916s - loss: 0.2899 - acc: 0.8742 - val_loss: 0.4441 - val_acc: 0.8100\n",
      "Epoch 7/7\n",
      "99000/99000 [==============================] - 1894s - loss: 0.2469 - acc: 0.8962 - val_loss: 0.4747 - val_acc: 0.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18a7f55e90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit([data1,data2],label,validation_split=0.01,nb_epoch=5, batch_size=64)\n",
    "import datetime, time, json\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "#print(\"Starting training at\", datetime.datetime.now())\n",
    "#t0 = time.time()\n",
    "#callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "model.fit([data1,data2],\n",
    "                    label,\n",
    "                    epochs=7,\n",
    "                    validation_split=0.01,\n",
    "                    shuffle=True,\n",
    "                    batch_size=128)\n",
    "#t1 = time.time()\n",
    "#print(\"Training ended at\", datetime.datetime.now())\n",
    "#print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "5289\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_V2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model_V2.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "  \n",
    "json_file = open(\"model_V2.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model_V2.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "Prediction =  np.argmax(loaded_model.predict([dev_data1,dev_data2]), axis = 1)\n",
    "print float(sum(dev_label==Prediction))/len(dev_label)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0757002271\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
